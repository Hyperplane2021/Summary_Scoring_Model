# -*- coding: utf-8 -*-
"""Summary_Scoring_Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lPBx4-mBzYnyycLp2zC6ZOCiI_uo6NRE
"""

!pip install flair
!pip install transformers

import pandas as pd
import numpy as np
from transformers import pipeline
import flair
from flair.embeddings import TransformerDocumentEmbeddings
from flair.data import Sentence
from sklearn.metrics.pairwise import cosine_similarity
import torch

class sumscore:
  '''
  Bertopic inspired me. The summarisation model uses the BART-LARGE-CNN of meta.
  After summarizing the text, the embedding is performed using a transformer model.
  
  Then, compare the embeddings from original document and summarized document using 
  cosine similarity.
  '''
  def __init__(self, model_name="all-mpnet-base-v2"):
    self.model = TransformerDocumentEmbeddings('sentence-transformers/'+str(model_name))
  
  def sum_score(self, docs, summary =None):
    if summary==None:

      device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

      summarizer = pipeline("summarization", model="facebook/bart-large-cnn", device=device)
      summary = summarizer(docs)[0]['summary_text']
    
    doc_sen = Sentence(docs)
    sum_sen = Sentence(summary)

    self.model.embed(doc_sen)
    self.model.embed(sum_sen)

    doc_em = np.array(doc_sen.get_embedding().cpu()).reshape(1,-1)
    sum_em = np.array(sum_sen.get_embedding().cpu()).reshape(1,-1)
    score = cosine_similarity(doc_em, sum_em)

    return {'summary':summary, 'score':score.item()}